\section{Introduction}
High-Level Synthesis (HLS) automates the process of creating digital hardware circuits from algorithms written in high-level languages.  
Modern HLS tools~\cite{xilinx_vivado_hls,intel_hls,canis2013legup} use the same front-end as the traditional software compilers.  
They rely on traditional compiler techniques to optimize the input program's intermediate representation (IR) and produce circuits in the form of RTL code from the IRs. 
Thus, the quality of compiler front-end optimizations directly impacts the performance of HLS-generated circuit. 
Meanwhile, optimizing a program is a notoriously difficult task for the compiler. 
Programs must be in just the right form for an optimization pass to recognize it---a task which a programmer might be able to easily do, but is often difficult for the compiler.
Moreover, a programmer might have to perform different optimizations depending on the hardware platform.
Despite a decade or so of research, an expert designer can still produce RTL that outperforms the results of HLS, though it incurs huge costs both in terms of time and human capital. 
% The goal of this paper is to work towards an intelligent HLS frontend that automatically decides which  optimizations and in which order to apply. 


%The main task of a compiler is to take a program written in a high level language and produce optimized executable code to run on specific platforms. This is a notoriously difficult task. 
%To guarantee they produce correct code, compilers make conservative decisions, which can lead to missing optimizations. \JENNY{This is not the problem we try to solve as we didn't modify the passes to be more intrusive}
%Thus, despite decades of research, an expert programmer can still produce code that outperforms a compiler. Unfortunately, manually optimizing code incurs huge costs both in terms of time and human capital. 

%Compilers have been successful in accomplishing two distinct goals: they have automated many expert-programmer techniques and have allowed for programmers to write high-level and algorithmically elegant programs. However, the benefits that compilers provide do not come without cost. Programs must be in just the right form for an optimization pass to recognize it -- a task which a programmer might be able to easily do, but is often difficult for the compiler. Rather than risk producing incorrect code, the compiler must act conservatively, leading to many missed optimizations. Thus, a program manually optimized by a programmer may often outperform a program optimized by a compiler, though at a significant cost both in time and intellectual capital.


The process of HLS compilation consists of a sequence of analysis and optimization phases that are applied to the program. 
Each phase consumes the output of the previous phase, and generates a modified version of the program for the next phase. Unfortunately, these phases are not commutative which makes the order in which these phases are applied critical to the performance of the output.  
%, each of them A compiler is composed of analyses and optimizations which are simply transformations that modify programs. Unfortunately, since optimizations do not generally commute, it is crucial to apply optimizations in the proper order. 


Consider the program in Figure \ref{fig:norm1}, which normalizes a vector. Without any optimizations the \verb|norm| function will take $\Theta(n^2)$ to normalize a vector. However, a smart compiler will implement the \textit{loop invariant code motion (LICM)}\cite[Sec.~13.2]{Muchnick97} optimization, which allows it to move the call to \verb|mag| above the loop, resulting in the code on the left column in Figure \ref{fig:norm2}. This brings the runtime down to $\Theta(n)$ -- a big speedup improvement. Another optimization the compiler could perform is \textit{(function) inlining}\cite[Sec.~15.2]{Muchnick97}. With inlining, a call to a function is simply replaced with the body of the function, reducing the overhead of the function call. Applying inlining to the code 
%in Figure \ref{fig:norm2}, 
will result in the code in the right column of Figure~\ref{fig:norm2}.
\begin{figure}[h]
     \centering
         \centering
\begin{minted}[fontsize=\footnotesize]{c}
__attribute__((const))
double mag(const double *A, int n) {
    double sum = 0;
    for(int i=0; i<n; i++){
        sum += A[i] * A[i];
    }
    return sqrt(sum);
}
void norm(double *restrict out,
          const double *restrict in, int n) {
    for(int i=0; i<n; i++) {
        out[i] = in[i] / mag(in, n);
    }
}
\end{minted}
\vspace*{-0.2cm}
\caption{A simple program to normalize a vector. \vspace*{-0.1cm}}
    \label{fig:norm1}
\end{figure}
\begin{figure*}[h]
     \centering
     \begin{subfigure}[b]{0.46\textwidth}
         \centering
\begin{minted}[fontsize=\footnotesize]{c}
void norm(double *restrict out,
          const double *restrict in, int n) {
    double precompute = mag(in, n);
    for(int i=0; i<n; i++) {
        out[i] = in[i] / precompute;
    }
}
\end{minted}
    %\label{fig:norm2}
\end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.46\textwidth}
         \centering
\begin{minted}[fontsize=\footnotesize]{c}
void norm(double *restrict out,
          const double *restrict in, int n) {
    double precompute, sum = 0;
    for(int i=0; i<n; i++){
        sum += A[i] * A[i];
    }
    precompute = sqrt(sum);
    for(int i=0; i<n; i++) {
        out[i] = in[i] / precompute;
    }
}
\end{minted}
     \end{subfigure}
     \vspace{-0.2cm}
     \caption{Progressively applying LICM (left) then inlining (right) to the code in Figure~\ref{fig:norm1}.}
    \label{fig:norm2}
    \vspace{-0.4cm}
\end{figure*}
\begin{figure*}[!h]
     \centering
     \begin{subfigure}[b]{0.46\textwidth}
         \centering
\begin{minted}[fontsize=\footnotesize]{c}
void norm(double *restrict out,
          const double *restrict in, int n) {
    for(int i=0; i<n; i++) {
        double sum = 0;
        for(int j=0; j<n; j++){
            sum += A[j] * A[j];
        }
        out[i] = in[i] / sqrt(sum);
    }
}
\end{minted}
    %\label{fig:norm4}
\end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.46\textwidth}
         \centering
\begin{minted}[fontsize=\footnotesize]{c}
void norm(double *restrict out,
          const double *restrict in, int n) {
    double sum;
    for(int i=0; i<n; i++) {
        sum = 0;
        for(int j=0; j<n; j++){
            sum += A[j] * A[j];
        }
        out[i] = in[i] / sqrt(sum);
    }
}
\end{minted}
     \end{subfigure}
          \vspace{-0.3cm}
     \caption{Progressively applying inlining (left) then LICM (right) to the code in Figure~\ref{fig:norm1}.}
     \label{fig:norm3}
     \vspace{-0.5cm}
\end{figure*}



Consider applying these optimization passes in the opposite order: first inlining then LICM. After inlining, we get the code on the left of Figure~\ref{fig:norm3}. Once again we get a modest speedup, having eliminated $n$ function calls, though our runtime is still $\Theta(n^2)$. If the compiler afterwards attempted to apply LICM, we would find the code on the right of Figure~\ref{fig:norm3}. LICM was able to successfully move the allocation of sum outside the loop. However, it was unable to move the instruction setting \verb|sum=0| outside the loop, as doing so would mean that all iterations excluding the first would have a garbage value for sum. Thus, the internal loop could not be moved out.

Consequently, having the wrong phase ordering for a program can mean the difference between one's program running in $\Theta(n^2)$ versus $\Theta(n)$. It is thus crucial to determine the optimal phase ordering for the program for it to run efficiently. Unfortunately, not only is this a difficult task, but the optimal phase ordering may vary from program to program.
The goal of this paper is to provide a mechanism for automatically determining good phase orderings for HLS programs to optimize for the circuit speed. 
%targeting hardware designs.

Recent advancements in deep reinforcement learning (RL)~\cite{sutton1998} offer opportunities to address the phase ordering challenge. 
In deep RL algorithms, software agents take actions in an environment to optimize accumulative rewards. 
%Generally, deep RL algorithms interact with the environment and are rewarded based on their actions. 
Based on these rewards and environment observations, statistical machine learning is applied to estimate the long-term benefit of the actions and optimize these actions accordingly. 
%In this work, we explore the benefits of applying deep RL algorithms to improve the quality of results (QoR) of HLS. 
Significant challenges exist in understanding how to formulate the phase ordering optimization problem in an RL framework. 
In this paper, we investigate two techniques to characterize the optimization state.
One is to directly use salient features from the program.
The other one is to use the applied optimizations as features without any information from the programs. 
We implement a framework that takes a group of programs and quickly finds a phase ordering that produces better quality of results.
Our main contributions are:
\begin{itemize}
    %\item Modified the LLVM compiler to extract 
    %program features that describe the program structure.  %describe the number of different operations from a specific HLS program.
    \item An analysis of two methods to represent program state in the RL setting: (1) program features and (2) a histogram of previously applied passes. We conclude why the later approach is more efficient in achieving better algorithm optimization time and circuit cycle count.
    \item A framework that integrates the compiler with the deep RL algorithms. This framework takes a group of programs as an input and robustly generates the optimal sequence of passes to apply. 
    %\item A study of our approach in multiple RL settings with significant performance and runtime improvements versus state-of-the-art approaches for compiler optimization.
    \item An evaluation and comparison of multiple RL algorithms against state-of-the-art approaches for compiler phase-ordering showing that RL runs one to two orders of magnitude faster and achieves $16\%$ better performance over -O3.
\end{itemize}

%The rest of the paper is organized a follows. Section~\ref{sec:bg} gives a brief background on compiler optimization, RL and related works. In Sections \ref{sec:framework} and \ref{sec:DRLA} the implemented framework and the deep RL architecture implementation are described, respectively. The framework is evaluated in Section~\ref{sec:results}, compared to state-of-the-art approaches for compiler optimization and a thorough analysis is presented. The paper is concluded in Section~\ref{sec:conc}.