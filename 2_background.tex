\section{background} % 1.5 pages
\label{sec:bg}
%\subsection{Compiler Optimization}
%Compilers often package groups of optimizations into ``optimization levels'' -O0 (no optimization), -O1 (some optimization) –O2 (more optimization), -O3 (most optimization) for ease. 
%While these optimization levels offer a simple set of choices for developers, they are somewhat arbitrarily chosen by compiler-designers and often most benefit the runtime of certain benchmark programs. 
%Often, -O3 will produce worse\TODO{Ameer: Jenny, why worse?} performance than -O2 on the same program. 
%While these optimization levels offer a simple set of choices for developers, they are handpicked by the compiler-designers and often benefit certain groups of benchmark programs. 
%The compiler community has attempted to address the issue by selecting a particular set of compiler optimization passes on a per-program or per-target basis for software\TODO{Ameer: no mention of hardware and focus on software}
%\JENNY{This is sw only as there is not much work on HLS, for hw it is mention in the related works}, with a few highly-cited examples \cite{triantafyllis2003compiler,almagor2004finding,pan2006fast}.

\subsection{Compiler Phase-ordering}
Compilers execute optimization passes to transform programs into more efficient forms to run on various hardware targets.
Groups of optimizations are often packaged into ``optimization levels'' -O0 (no optimization), -O1 (some optimization) –O2 (more optimization), -O3 (most optimization) for ease. 
While these optimization levels offer a simple set of choices for developers, they are handpicked by the compiler-designers and often most benefit certain groups of benchmark programs. 
The compiler community has attempted to address the issue by selecting a particular set of compiler optimizations on a per-program or per-target basis for software\cite{triantafyllis2003compiler,almagor2004finding,pan2006fast}.


Since the search space of phase-ordering is too large for exhaustive search, many heuristics have been proposed to explore the space by using machine learning. 
Huang~\textit{et al.} tried to address this challenge for HLS applications by using modified greedy algorithms~\cite{huang2013effect}\cite{huang2015effect}. %It achieved 16\% improvement vs -O3 on HLS generated circuits.  %to apply a new optimization to an existing sequence and keep the best one or three sequences at each 
In~\cite{agakov2006using} both independent and Markov models were applied to automatically target an optimized search space for iterative methods to improve the search results.
In~\cite{2003Stephenson}, genetic algorithms were used to tune heuristic priority functions for three compiler optimization passes. 
Milepost\cite{fursin2011milepost} used machine learning to determine the set of passes to apply to a given program, based on a static analysis of its features. It achieved 11\% execution time improvement over -O3, for the ARC reconfigurable processor on the MiBench program suite1.
In~\cite{2012Kulkarni} the challenge was formulated as a Markov process and supervised learning was used to predict the next optimization, based on the current program state.
Wang \textit{et al.}~\cite{wang2018}, described a relationship between machine learning and compiler optimization, where the program features are used as an observation, without explicitly providing results. In this work, we tried using the program features as an observation but found that it faces several challenges explained in Section~\ref{sec:DRLA}.
\subsection{Reinforcement Learning}

Reinforcement learning (RL) is a machine learning approach in which an agent continually interacts with the environment~\cite{kaelbling1996reinforcement}. In particular, the agent observes the state of the environment, and based on this observation takes an action. The goal of the RL agent is then to compute a policy--a mapping between the environment states and actions--that maximizes a long term reward. 
%by taking actions. These actions are generally the output of a policy that takes as an input a state/observation. The objective of RL is to find a policy that maximizes the reward an agent receives while interacting with the environment.

RL can be viewed as a stochastic optimization solution for solving Markov Decision Processes (MDPs)~\cite{bellman1957}, when the MDP is not known. An MDP 
%models the system with an agent that wants to optimize a given objective (such as the score in a game) by making a sequence of decisions. Given the current state, a decision is made, which leads to a new state. More formally, the Markov decision process 
is defined by a tuple with four elements:
${S,A,P(s,a),r(s,a)}$ where $S$ is the set of states of the environment, $A$ describes the set of actions or transitions between states, $s' {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
 P(s,a)$ describes the probability distribution of next states given the current state and action and $r(s,a):S \times A \rightarrow R$ is the reward of taking action $a$ in state $s$. Given an MDP, the goal of the agent is to gain the largest possible aggregate reward. The objective of an RL algorithm associated with an MDP is to find a decision policy $\pi^*:s\rightarrow A$ that achieves this goal for that MDP:
 %, by mapping states to actions, that maximize the reward:
\begin{multline}
\label{eq:MDP}
    \pi^* = \argmaxA_\pi E_{\tau{\raise.05ex\hbox{$\scriptstyle\mathtt{\sim}$}}\rho_{\pi(\tau)}} \left[\sum_{t}^{}r(s_t,a_t) \right] = \\
    \argmaxA_\pi \sum_{t=1}^{T}E_{(s_t,a_t){\raise.05ex\hbox{$\scriptstyle\mathtt{\sim}$}}\rho_{\pi(s_t,a_t)}}\left[r(s_t,a_t)\right].
\end{multline}

Deep RL leverages a neural network to learn the policy (and sometimes the reward function). Recently, Deep RL has achieved impressive results, such as learning to play 49 Atari games with human-level capabilities~\cite{mnih2015}, and defeating the Go world champion~\cite{silver2016}. Over the past couple of years, a plethora of deep RL techniques have been proposed~\cite{mnih2016,ross2011,sutton2000,watkins1992q}. In this paper, we focus on two algorithms: Policy Gradient (PG)~\cite{sutton2000} and Deep Q-Network (DQN), commonly referred to as Q-Learning~\cite{watkins1992q}. We chose these algorithms as they are simple, mature, and adequate to solve the problem. 

PG computes the policy directly by differentiating the aggregate reward \textit{i.e.}, the term in Equation~\ref{eq:MDP}:
\begin{multline}
\label{eq:policygradient}
    \nabla_\theta J =  \nabla_\theta E_{\tau{\raise.05ex\hbox{$\scriptstyle\mathtt{\sim}$}}\rho_{\pi(\tau)}} \left[\sum_{t}^{}r(s_t,a_t) \right] = \\
    E_{\tau{\raise.05ex\hbox{$\scriptstyle\mathtt{\sim}$}}\rho_{\pi(\tau)}} \left[(\sum_{t}^{}\nabla_\theta log\pi_\theta(a_t|s_t))(\sum_{t}^{}r(s_t,a_t))\right] \approx \\
    \frac{1}{N}\sum_{i=1}^{N} \left[ (\sum_{t}^{}\nabla_\theta log\pi_\theta(a_{i,t}|s_{i,t}))(\sum_{t}^{}r(s_{i,t},a_{i,t})) \right]
\end{multline}
and then updating the network parameters (weights) in the direction of the gradient:
\begin{equation}
    \theta \leftarrow \theta+\alpha\nabla_\theta J,
\end{equation}
where $\alpha$ is called the learning rate. PG is an on-policy method in that it uses only the current policy to compute the new policy.

In contrast, DQN is an off-policy method which takes partially random actions to explore the state space. The DQN's goal is to find which actions will maximize future rewards from a given state. To do so DQN computes a $Q$-function, $Q(s_i, a_i)$ that predicts the future overall reward of taking action $a_i$ from state $s_i$. To compute this $Q$-function, DQN uses a neural network paramterized by weights $\phi$). More formally:

\begin{equation}
\label{eq:Dqn}
    y_i = r(s_i,a_i) + \argmaxA_{a'_i}Q_\phi(s'_i,a'_i)\\
\end{equation}
\begin{equation}
    \phi \leftarrow \argminA_{\phi}\sum_i||Q_\phi(s_i,a_i)-y_i||^2,
\end{equation}
\noindent
where $y_i$ is the target result, $a_i$ and $s_i$ are the current action and state respectively, $a'_i$ and $s'_i$ are the next action and state respectively, and $r(s_i,a_i)$ is the reward for taking action $a_i$ at state $s_i$. On top of that, the policy is basically defined as follow:
\begin{equation}
    \pi_\theta(a_t|s_t) =
    \begin{cases}
    1 & \text{if } a_t =\argmaxA_{a_t}Q_\phi(s'_t,a'_t)\\
    0              & \text{otherwise}
\end{cases}
\end{equation}
